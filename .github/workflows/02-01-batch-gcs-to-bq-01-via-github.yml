name: Google Provided Batch Job - GCS to BigQuery - via Github Actions

on: workflow_dispatch # to be triggered manually

env:
  # Environment Variables for Cloud Storage
  GCS_BUCKET_NAME: gs://dataflow-bucket-gcp-practice-project-aman
  GCS_FOLDER_LOCATION: 02-google-template/01-batch-jobs/gcs-to-bigquery/01-text-file/01-via-github-actions

  # Environment Variables for Job
  JOB_NAME: batch-gcs-to-bigquery-via-github
  REGION: us-central1

  # Environment Variables for Javascript
  JS_FUNCTION_NAME: transform
  JS_FILE_NAME: user-function/transform.js

  # Environment Variables for BigQuery
  BQ_SCHEMA_FILE_NAME: config/bq-schema.json
  BQ_TABLE_NAME: gcp-practice-project-aman:raw_layer.dataflow_template_table

  # Environment Variables for Input Data
  INPUT_FILE_NAME: data/sample_input.csv

jobs:
  run-template:
    name: Dataflow Template
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: ${{ env.GCS_FOLDER_LOCATION }}

    permissions:
      contents: read
      id-token: write

    steps:
      - name: Checkout
        uses: actions/checkout@v3

      # authenticate with gcloud
      - name: Authenticate to Google Cloud
        id: auth
        uses: google-github-actions/auth@v1
        with:
          token_format: access_token
          workload_identity_provider: ${{ secrets.WIF_PROVIDER }}
          service_account: ${{ secrets.SA_EMAIL }}

      # Set up Cloud SDK
      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v1

        # dump the files to cloud storage
      - name: Write Files to Cloud Storage
        run: |
          gsutil cp ${{ env.JS_FILE_NAME }} ${{ env.GCS_BUCKET_NAME }}/${{ env.GCS_FOLDER_LOCATION }}/${{ env.JS_FILE_NAME }}
          gsutil cp ${{ env.BQ_SCHEMA_FILE_NAME }} ${{ env.GCS_BUCKET_NAME }}/${{ env.GCS_FOLDER_LOCATION }}/${{ env.BQ_SCHEMA_FILE_NAME }}
          gsutil cp ${{ env.INPUT_FILE_NAME }} ${{ env.GCS_BUCKET_NAME }}/${{ env.GCS_FOLDER_LOCATION }}/${{ env.INPUT_FILE_NAME }}

      # Build and Deploy the Dataflow Job
      - name: Run Google Template Job
        run: |
          gcloud dataflow jobs run ${{ env.JOB_NAME }} \
          --gcs-location  gs://dataflow-templates/latest/GCS_Text_to_BigQuery \
          --region ${{ env.REGION }} \
          --staging-location ${{ env.GCS_BUCKET_NAME }}/staging \
          --service-account-email ${{ secrets.SA_EMAIL }} \
          --parameters \
          javascriptTextTransformFunctionName="${{ env.JS_FUNCTION_NAME }}",\
          JSONPath="${{ env.GCS_BUCKET_NAME }}/${{ env.GCS_FOLDER_LOCATION }}/config/${{ env.BQ_SCHEMA_FILE_NAME }}",\
          javascriptTextTransformGcsPath="${{ env.GCS_BUCKET_NAME }}/${{ env.GCS_FOLDER_LOCATION }}/user-function/${{ env.JS_FILE_NAME }}",\
          inputFilePattern="${{ env.GCS_BUCKET_NAME }}/${{ env.GCS_FOLDER_LOCATION }}/data/${{ env.INPUT_FILE_NAME }}",\
          outputTable="${{ env.BQ_TABLE_NAME }}",\
          bigQueryLoadingTemporaryDirectory="${{ env.GCS_BUCKET_NAME }}/temp"
