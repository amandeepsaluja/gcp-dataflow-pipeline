{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class to Extract JSON Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom DoFn to extract the specified fields from nested JSON\n",
    "class ExtractFields(beam.DoFn):\n",
    "    def __init__(self, fields_to_extract):\n",
    "        self.fields_to_extract = fields_to_extract\n",
    "\n",
    "    def process(self, element):\n",
    "        import json\n",
    "\n",
    "        try:\n",
    "            # creating a list of fields to extract from string\n",
    "            fields_to_extract = self.fields_to_extract.split(\",\")\n",
    "\n",
    "            extracted_data = {}\n",
    "            for field in fields_to_extract:\n",
    "                # Split the field name by '.' to navigate nested structures\n",
    "                field_parts = field.split(\".\")\n",
    "                current_data = element\n",
    "                for part in field_parts:\n",
    "                    if part in current_data:\n",
    "                        current_data = current_data[part]\n",
    "                    else:\n",
    "                        # Field not found, skip this field\n",
    "                        current_data = None\n",
    "                        break\n",
    "\n",
    "                if current_data is not None:\n",
    "                    extracted_data[field] = current_data\n",
    "\n",
    "            if extracted_data:\n",
    "                yield extracted_data\n",
    "        except (json.JSONDecodeError, ValueError) as e:\n",
    "            # Handle JSON decoding errors here\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class to Add 2 new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddDatetimeAndDate(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        from datetime import datetime\n",
    "\n",
    "        new_element = dict(element)  # Create a copy of the original dictionary\n",
    "\n",
    "        current_time = datetime.now()\n",
    "\n",
    "        # Add a DATETIME column with the current timestamp\n",
    "        new_element[\"load_datetime\"] = current_time\n",
    "        new_element[\"load_date\"] = current_time.date()\n",
    "\n",
    "        yield new_element  # Emit the new dictionary with the added columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Pipeline with FPL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 38, 'code': 2367575, 'load_datetime': datetime.datetime(2023, 10, 17, 14, 28, 34, 935974), 'load_date': datetime.date(2023, 10, 17)}\n",
      "{'id': 40, 'code': 2367577, 'load_datetime': datetime.datetime(2023, 10, 17, 14, 28, 34, 936979), 'load_date': datetime.date(2023, 10, 17)}\n",
      "{'id': 32, 'code': 2367569, 'load_datetime': datetime.datetime(2023, 10, 17, 14, 28, 34, 936979), 'load_date': datetime.date(2023, 10, 17)}\n",
      "{'id': 34, 'code': 2367571, 'load_datetime': datetime.datetime(2023, 10, 17, 14, 28, 34, 936979), 'load_date': datetime.date(2023, 10, 17)}\n",
      "{'id': 35, 'code': 2367572, 'load_datetime': datetime.datetime(2023, 10, 17, 14, 28, 34, 937986), 'load_date': datetime.date(2023, 10, 17)}\n",
      "{'id': 39, 'code': 2367576, 'load_datetime': datetime.datetime(2023, 10, 17, 14, 28, 34, 937986), 'load_date': datetime.date(2023, 10, 17)}\n",
      "{'id': 33, 'code': 2367570, 'load_datetime': datetime.datetime(2023, 10, 17, 14, 28, 34, 938507), 'load_date': datetime.date(2023, 10, 17)}\n",
      "{'id': 36, 'code': 2367573, 'load_datetime': datetime.datetime(2023, 10, 17, 14, 28, 34, 938507), 'load_date': datetime.date(2023, 10, 17)}\n",
      "{'id': 37, 'code': 2367574, 'load_datetime': datetime.datetime(2023, 10, 17, 14, 28, 34, 938507), 'load_date': datetime.date(2023, 10, 17)}\n",
      "{'id': 31, 'code': 2367568, 'load_datetime': datetime.datetime(2023, 10, 17, 14, 28, 34, 939021), 'load_date': datetime.date(2023, 10, 17)}\n",
      "{'id': 38, 'code': 2367575, 'load_datetime': datetime.datetime(2023, 10, 17, 14, 28, 35, 775712), 'load_date': datetime.date(2023, 10, 17)}\n",
      "{'id': 40, 'code': 2367577, 'load_datetime': datetime.datetime(2023, 10, 17, 14, 28, 35, 775712), 'load_date': datetime.date(2023, 10, 17)}\n",
      "{'id': 32, 'code': 2367569, 'load_datetime': datetime.datetime(2023, 10, 17, 14, 28, 35, 775712), 'load_date': datetime.date(2023, 10, 17)}\n",
      "{'id': 34, 'code': 2367571, 'load_datetime': datetime.datetime(2023, 10, 17, 14, 28, 35, 775712), 'load_date': datetime.date(2023, 10, 17)}\n",
      "{'id': 35, 'code': 2367572, 'load_datetime': datetime.datetime(2023, 10, 17, 14, 28, 35, 775712), 'load_date': datetime.date(2023, 10, 17)}\n",
      "{'id': 39, 'code': 2367576, 'load_datetime': datetime.datetime(2023, 10, 17, 14, 28, 35, 775712), 'load_date': datetime.date(2023, 10, 17)}\n",
      "{'id': 33, 'code': 2367570, 'load_datetime': datetime.datetime(2023, 10, 17, 14, 28, 35, 775712), 'load_date': datetime.date(2023, 10, 17)}\n",
      "{'id': 36, 'code': 2367573, 'load_datetime': datetime.datetime(2023, 10, 17, 14, 28, 35, 775712), 'load_date': datetime.date(2023, 10, 17)}\n",
      "{'id': 37, 'code': 2367574, 'load_datetime': datetime.datetime(2023, 10, 17, 14, 28, 35, 775712), 'load_date': datetime.date(2023, 10, 17)}\n",
      "{'id': 31, 'code': 2367568, 'load_datetime': datetime.datetime(2023, 10, 17, 14, 28, 35, 775712), 'load_date': datetime.date(2023, 10, 17)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.portability.fn_api_runner.fn_runner.RunnerResult at 0x19b5e4a78e0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields_to_extract = \"id,code\"\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "\n",
    "    json_data = (\n",
    "        p | \"Read API\" >> beam.Create([\"https://fantasy.premierleague.com/api/fixtures/?event=4\"])\n",
    "        | \"HTTP GET\" >> beam.ParDo(lambda url: requests.get(url).json())\n",
    "        | \"Extract Fields\" >> beam.ParDo(ExtractFields(fields_to_extract)) \n",
    "        | \"AddDatetimeAndDate\" >> beam.ParDo(AddDatetimeAndDate())         \n",
    "        | \"Print\" >> beam.Map(print)\n",
    "    )\n",
    "\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Pipeline with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Buzz'}\n",
      "{'name': 'Trashy Blonde'}\n",
      "{'name': 'Berliner Weisse With Yuzu - B-Sides'}\n",
      "{'name': 'Pilsen Lager'}\n",
      "{'name': 'Avery Brown Dredge'}\n",
      "{'name': 'Electric India'}\n",
      "{'name': 'AB:12'}\n",
      "{'name': 'Fake Lager'}\n",
      "{'name': 'AB:07'}\n",
      "{'name': 'Bramling X'}\n",
      "{'name': 'Misspent Youth'}\n",
      "{'name': 'Arcade Nation'}\n",
      "{'name': 'Movember'}\n",
      "{'name': 'Alpha Dog'}\n",
      "{'name': 'Mixtape 8'}\n",
      "{'name': 'Libertine Porter'}\n",
      "{'name': 'AB:06'}\n",
      "{'name': 'Russian Doll – India Pale Ale'}\n",
      "{'name': 'Hello My Name Is Mette-Marit'}\n",
      "{'name': 'Rabiator'}\n",
      "{'name': 'Vice Bier'}\n",
      "{'name': 'Devine Rebel (w/ Mikkeller)'}\n",
      "{'name': 'Storm'}\n",
      "{'name': 'The End Of History'}\n",
      "{'name': 'Bad Pixie'}\n",
      "{'name': 'Buzz'}\n",
      "{'name': 'Trashy Blonde'}\n",
      "{'name': 'Berliner Weisse With Yuzu - B-Sides'}\n",
      "{'name': 'Pilsen Lager'}\n",
      "{'name': 'Avery Brown Dredge'}\n",
      "{'name': 'Electric India'}\n",
      "{'name': 'AB:12'}\n",
      "{'name': 'Fake Lager'}\n",
      "{'name': 'AB:07'}\n",
      "{'name': 'Bramling X'}\n",
      "{'name': 'Misspent Youth'}\n",
      "{'name': 'Arcade Nation'}\n",
      "{'name': 'Movember'}\n",
      "{'name': 'Alpha Dog'}\n",
      "{'name': 'Mixtape 8'}\n",
      "{'name': 'Libertine Porter'}\n",
      "{'name': 'AB:06'}\n",
      "{'name': 'Russian Doll – India Pale Ale'}\n",
      "{'name': 'Hello My Name Is Mette-Marit'}\n",
      "{'name': 'Rabiator'}\n",
      "{'name': 'Vice Bier'}\n",
      "{'name': 'Devine Rebel (w/ Mikkeller)'}\n",
      "{'name': 'Storm'}\n",
      "{'name': 'The End Of History'}\n",
      "{'name': 'Bad Pixie'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.portability.fn_api_runner.fn_runner.RunnerResult at 0x19b596b8cd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields_to_extract = \"name\"\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "\n",
    "    json_data = (\n",
    "        p | \"Read API\" >> beam.Create([\"https://api.punkapi.com/v2/beers\"])\n",
    "        | \"HTTP GET\" >> beam.ParDo(lambda url: requests.get(url).json())\n",
    "        | \"Extract Fields\" >> beam.ParDo(ExtractFields(fields_to_extract))\n",
    "        # | \"AddDatetimeAndDate\" >> beam.ParDo(AddDatetimeAndDate())        \n",
    "        | \"Print\" >> beam.Map(print)\n",
    "    )\n",
    "\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
